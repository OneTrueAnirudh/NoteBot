import sys
from GUI import browse_files
from chunking import PDF_parser_chunker
from vectorize import process_and_upload_chunks
from slide_keywords import read_pptx, llm_keywords, get_keyword_vectors
from qdrant_utils import search_vectors, initialize_qdrant_client_cloud, search_res_util
from doc_gen import TextSummarizer


def main():
    # Upload textbook and slides via GUI
    textbook, slides = browse_files()
    print("Textbook Path:", textbook)
    print("Slides Path:", slides)

    # Chunk the textbook (using PDF_parser_chunker)
    chunks = PDF_parser_chunker(textbook).Chunker()

    # Get vector embeddings for the chunks and upload to Qdrant Cloud
    collection_name = "NoteBot"  # You can specify your collection name here
    formatted_embeddings = process_and_upload_chunks(chunks, collection_name)

    # Print the uploaded vectors and their metadata (text chunk)
    # for point in formatted_embeddings:
    #     print(f"ID: {point['id']}")
    #     print(f"Vector: {point['vector']}")
    #     # Encode the payload text to handle special characters
    #     print(f"Metadata (Text Chunk): {point['payload']['text'].encode('utf-8', 'replace').decode('utf-8')}")
    #     print("-" * 50)

    # Extract text from the slides and generate keywords
    slide_text = read_pptx(slides)
    keywords = llm_keywords(slide_text)
    print("Keywords Extracted:", keywords)

    # Get vector embeddings for the extracted keywords
    keyword_vectors = get_keyword_vectors(keywords)
    print(f"Number of Keyword Vectors: {len(keyword_vectors)}")

    # Perform similarity search on Qdrant Cloud (using keyword vectors)
    client = initialize_qdrant_client_cloud()

    # Dictionary to store the keywords and corresponding chunks
    key_chunk = {}

    # Loop through both keywords and their corresponding keyword vectors
    for keyword, keyword_vector in zip(keywords, keyword_vectors):
        # print("-" * 200)
        num = ["1. ","2. ","3. ","4. ","5. ","6. ","7. ","8. ","9. ","10. ","11. ","12. ","13. ","14. ","15. "]
        for i in num:
            if i in keyword:
                keyword = keyword[3:]
                break
        if "Generated by" not in keyword:
            search_results = search_vectors(client, collection_name, query_vector=keyword_vector, keyword=keyword, limit=3)
            chunks_sim = search_res_util(search_results)  
            key_chunk[keyword] = chunks_sim
    
    
    # Document generation
    summarizer = TextSummarizer()
    proc_chunks = summarizer.process_chunks(key_chunk)
    summarizer.create_word_document(proc_chunks, name="ind.docx")
    print("Notes have been generated successfully!")
    
    sys.stdout.reconfigure(encoding='utf-8')


if __name__ == "__main__":
    main()
